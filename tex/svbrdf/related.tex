\section{Related work}
\label{sec:svbrdf:related}

\paragraph{Reflectance capture.}

Acquiring material data from physical measurements is the goal of a broad range of methods.
Please refer to surveys \cite{weyrich2009principles,guarnera2016brdf,dong2019deep} for more comprehensive introduction to the related works.

Most reflectance capture approaches observe a material sample under varying viewing and lighting configurations. They differ in the number of light patterns required and their types such as moving linear light \cite{gardner2003linear,ren2011pocket}, Gray code patterns \cite{guarnera2016brdf}, spherical harmonic illumination \cite{ghosh2009estimating}, and Fourier patterns \cite{aittala2013practical}.

Methods have also been proposed for material capture ``in the wild'', i.e., under uncontrolled environment conditions with commodity hardware, typically captured with a hand-held mobile phone with flash illumination. Some of these methods impose strong priors on the materials, such as linear combinations of basis BRDFs \cite{hui2017reflectance,xu2016minimal} (where the basis BRDFs can come from the measured data \cite{matusik2003data}). Later work by Aittala et al. \cite{aittala2015two,aittala2016reflectance} estimated per-pixel parameters of stationary spatially-varying SVBRDFs from two-shot and one-shot photographs.
In the latter case, the approach used a neural Gram-matrix texture descriptor based on the texture synthesis and feature transfer work of Gatys \cite{gatys2015neural,gatys2016image} to compare renderings with similar texture patterns but without pixel alignment.

More recently, deep learning-based approaches have demonstrated remarkable progress in the quality of SVBRDF estimates from single images (usually captured under flash illumination) \cite{li2017modeling,deschaintre2018single,li2018materials}. These methods train deep convolutional neural networks with large datasets of artistically created SVBRDFs, and with a combination of losses that evaluate the difference in material maps and renderings from the dataset ground truth.

Deschaintre \cite{deschaintre2019flexible} extended the single-shot approach to multiple images. The key idea is to extract features from the input images with a shared encoder, max-pooling the features and decoding the final maps from the pooled features. This architecture has the benefit of being independent of the number of inputs, while also not requiring explicit light position information. In our experience, this approach produces smooth, plausible maps with low artifacts; however, re-rendering the maps tends to be not as close to the target measurements because the network cannot ``check'' its results at runtime. Moreover, we find that especially on real data, this method also has strong biases such as dark diffuse albedo maps and exaggerating surface normals (especially along strong image gradients that might be caused by albedo variations). We believe this is not due to any technical flaw; the method may be reaching the limit of what is possible using current feed-forward convolutional architectures and currently available datasets.

Gao et al. \cite{gao2019deep} introduced an inverse rendering-based material capture approach that optimizes for material maps to minimize error with respect to the captured images. Since this is an under-constrained problem, they propose optimizing over the latent space of a learned material auto-encoder network to minimize rendering error. This approach has the benefit of explicitly matching the appearance of the captured image measurements, while also using the auto-encoder as a material ``prior''. Moreover, the encoder and decoder are fully convolutional, which has the advantage of resolution independence.
However, we find that the convolutional nature of this model also has the disadvantage of only providing local regularization and not capturing global patterns in the material, such as the long-range spatial patterns and correlations between the different material parameter maps. As a result, this method relies on previous methods (for example, Deschaintre et al. \cite{deschaintre2018single}) to provide a good initialization, without which it can converge to poor results.
In contrast, our MaterialGAN is a more globally robust latent space and produces higher quality reconstructions without requiring accurate initializations, though it is no longer resolution-independent.

\paragraph{Generative adversarial networks.}

GANs \cite{goodfellow2014generative} have become extremely successful in the past few years in various domains, including images \cite{radford2015unsupervised}, video \cite{tulyakov2018mocogan}, audio \cite{donahue2018synthesizing}, and 3D shapes \cite{li2019synthesizing}. A GAN typically consists of two competing networks; a generator, whose goal is to produce results that are indistinguishable from the real data distribution, and a discriminator, whose job is to learn to identify generated results from real ones. For generating realistic images (especially of human faces), there has been a sequence of improved models and training strategies, including ProgressiveGAN \cite{karras2018progressive}, StyleGAN \cite{karras2019style} and StyleGAN2 \cite{karras2020analyzing}. StyleGAN2 in particular is the state-of-the-art GAN model and our work is based on its architecture, modified to output more channels.

Recently, GANs have also been used to solve inverse problems \cite{bora2017compressed,asim2020invertible,o2019learning}.
In computer graphics and vision, this work has focused on embedding images into the latent space, with the goal of editing the images in semantically meaningful ways via latent vector manipulations \cite{zhu2016generative}. This embedding requires solving an optimization problem to find the latent vector.
More recent work such as Image2StyleGAN \cite{abdal2019image2stylegan} and Image2StyleGAN++ \cite{abdal2020image2stylegan++} has looked at problem of embedding images specifically into the the StyleGAN latent space. While these methods focus on projecting portrait images into face-specific StyleGAN models, we find their analysis can be adapted to our problem.
We build on this to propose a GAN embedding-based inverse rendering approach.
