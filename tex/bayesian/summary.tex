\section{Summary Functions}
\label{sec:bayesian:summary}

To solve the parameter estimation problem using Eq. \eqref{eq:approx}, a key ingredient is the appearance-match relation.
Unfortunately, we cannot use simplistic image difference metrics such as the L2 or L1 norms.
This is because the features (bumps, scratches, flakes, yarns, etc.) in the images of real-world materials are generally misaligned, even when the two images represent the same material.
In procedural modeling, as shown in Figure \ref{fig:bayesian:sum_func}, with irregularities created differently using $\bmz_1$ and $\bmz_2$, the same procedural model parameters $\bmtheta$ can yield slightly different results $f(\bmtheta; \bmz_1)$ and $f(\bmtheta, \bmz_2)$.

\input{tex/bayesian/fig/sum_func}

To overcome this challenge, we use the concept of a \emph{summary function}, which abstracts away the unimportant differences in the placement of the features, and summarizes the predicted and target images into smaller vectors whose similarity can be judged with simple metrics like L2 distance.
We define an image summary function $\calS$ to be a continuous function that maps an image of a material ($\bmIt$ or $\bmIs$) into a vector in $\bbR^k$. An idealized summary function would have the property that
\begin{equation}
	\calS(f(\bmtheta_1, \bmz_1)) = \calS(f(\bmtheta_2, \bmz_2)) \ \Leftrightarrow \ \bmtheta_1 = \bmtheta_2.
\end{equation}
That is, applying the summary function would abstract away from the randomness $\bmz$ and the difference between the two summary vectors would be entirely due to different material properties $\bmtheta$. Practical summary functions will satisfy the above only approximately. However, a good practical summary function will embed images of the same material close to each other, and images of different materials further away from each other. Below we discuss several techniques for constructing summary functions.

\paragraph{Neural summary function.}
Gatys et al. \cite{gatys2015neural,gatys2016image} introduced the idea of using the features of an image classification neural network (usually VGG \cite{simonyan2014very}) as a descriptor $T_G$ of image texture (or style). Optimizing images to minimize the difference in $T_G$ (combined with other constraints) allowed Gatys et al. to produce impressive, state-of-the art results for parametric texture synthesis and style transfer between images. While further work  has introduced improvements \cite{risser2017stable}, we find that the original version from Gatys et al. works already well in our case.

Aittala et al. \cite{aittala2016reflectance} introduced this technique to capturing material parameter textures (albedo, roughness, normal and specular maps) of stationary materials. They optimized for a $256 \times 256$ stationary patch that matches the target image in various crops, using a combination of $T_G$ and a number of special Fourier-domain priors. In our case (for procedural materials), we find that the neural summary function $T_G$ works even more effectively; we can simply apply it to the entire target or simulated images (not requiring crops nor Fourier-domain priors).

Specifically, define the Gram matrix $G$ of a set of feature maps $F_1, \cdots, F_n$ such that it has elements
\begin{equation}
	G_{ij} = \mbox{mean}(F_i \cdot F_j),
\end{equation}
where the product $F_i \cdot F_j$ is element-wise. $T_G$ is defined as the concatenation of the flattened Gram matrices computed for the feature maps before each pooling operation in VGG19. Note that the size of the Gram matrices depends on the number of feature maps (channels), not their size; thus $T_G$ is independent of input image size.

\paragraph{Statistics and Fourier transforms of image bins.}
While the neural summary function performs quite well, we find that in some cases we can improve upon it.
A simple idea for a summary function is to use the (RGB) mean of the entire image; an improvement is to subdivide the image into $k$ bins (regions) and compute the mean of each region. We found concentric bins perform well for isotropic materials, and vertical bins are appropriate for anisotropic highlights (e.g. brushed metal). Furthermore, we can additionally use a fast Fourier transform of the entire image or within bins. Note that automatic computation of derivatives is possible with the FFT, and supported by the PyTorch framework. In our current results, we use a summary function that combines the means and 1D FFTs of 64 vertical bins for the brushed metal example; all other examples use the neural summary function combined with simple image mean.
